{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd3d5a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f37a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.tracers import LangChainTracer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47245120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GenAI'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"LANGCHAIN_API_KEY\"]  \n",
    "os.environ[\"LANGCHAIN_PROJECT\"]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6db27098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracker\n",
    "tracer = LangChainTracer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0028db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "loader = TextLoader(\"data.txt\") \n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01cdbd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into smaller chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "splits = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90aad42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW\\AppData\\Local\\Temp\\ipykernel_21912\\384518551.py:2: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
      "C:\\Users\\BW\\AppData\\Local\\Temp\\ipykernel_21912\\384518551.py:4: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "# Creating embeddings for chunks\n",
    "embedding = OllamaEmbeddings(model=\"nomic-embed-text\") \n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=embedding, persist_directory=\"./chroma_db\")\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1df1a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating retriever\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bf021c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the propmt\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a Philosopher AI assistant. Use the following context to answer the user's question.\n",
    "    If you don't know the answer, say you don't know.\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5f4847c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW\\AppData\\Local\\Temp\\ipykernel_21912\\1674934915.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"gemma3:1b\")\n"
     ]
    }
   ],
   "source": [
    "# Setting up the LLM\n",
    "llm = Ollama(model=\"gemma3:1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3d923d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the chain with tracing\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, chain_type_kwargs={\"prompt\": prompt}, callbacks=[tracer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7455c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Philosopher AI is ready. Ask your questions!\n",
      "\n",
      "\n",
      "Answer: Philosophy is the art of questioning and exploring fundamental truths about existence, reason, and our understanding of the world. It seeks wisdom and helps us navigate lifeâ€™s complexities. \n",
      "\n",
      "\n",
      "Exiting... Goodbye!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPhilosopher AI is ready. Ask your questions!\\n\")\n",
    "while True:\n",
    "    try:\n",
    "        query = input(\"Ask (or 'exit' to quit): \").strip()\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"\\nExiting... Goodbye!\")\n",
    "            break\n",
    "\n",
    "        if not query:\n",
    "            print(\"Please enter a valid question.\\n\")\n",
    "            continue\n",
    "\n",
    "        result = qa_chain.invoke({\"query\": query})\n",
    "        print(\"\\nAnswer:\", result[\"result\"], \"\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {e}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
